{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "B62aj2RFYxNi",
        "outputId": "a41d0e45-918b-410f-fd36-c9fbdf1ded68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch 1/1, Batch 0/782, Loss: 2.4357717037200928\n",
            "Epoch 1/1, Batch 100/782, Loss: 1.9979636669158936\n",
            "Epoch 1/1, Batch 200/782, Loss: 2.032679557800293\n",
            "Epoch 1/1, Batch 300/782, Loss: 1.7990580797195435\n",
            "Epoch 1/1, Batch 400/782, Loss: 1.906455397605896\n",
            "Epoch 1/1, Batch 500/782, Loss: 1.5445990562438965\n",
            "Epoch 1/1, Batch 600/782, Loss: 1.5406376123428345\n",
            "Epoch 1/1, Batch 700/782, Loss: 1.5161807537078857\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the basic building blocks: Residual Block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = None\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Create a ResNet model\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "# Instantiate the model\n",
        "model = ResNet18()\n",
        "\n",
        "# You can now train this model using your dataset and an appropriate loss function and optimizer.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Define your data transforms, e.g., for data augmentation and normalization.\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# Load your dataset. Replace 'root' and other parameters with your data.\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "# Define a loss function (e.g., cross-entropy) and an optimizer (e.g., SGD).\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# Train the model\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
        "\n",
        "# Call the train function\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e6hxLA_aY45w",
        "outputId": "927edfd7-4a14-4147-b9e5-7edbe9f5e9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Test Accuracy: 44.64%\n",
            "Average Test Loss: 1.5221\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already defined the ResNet-18 model and training code as shown earlier.\n",
        "\n",
        "# Define the test dataset and data loader\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate the number of correct predictions\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct_predictions += predicted.eq(targets).sum().item()\n",
        "\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    # Calculate accuracy and average loss\n",
        "    accuracy = (correct_predictions / total_samples) * 100.0\n",
        "    average_loss = total_loss / len(test_loader)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Average Test Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Call the evaluation function\n",
        "evaluate(model, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM26v9hyk37_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}